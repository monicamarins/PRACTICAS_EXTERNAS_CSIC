{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "069bef26",
   "metadata": {},
   "source": [
    "In this notebook it will be explained how to get and plot the data from the USGS Earthquake Catalog. \n",
    "\n",
    "The USGS Earthquake Catalog is a database of earthquakes that have occurred around the world. Using the library \"libcomcat\" we can acces the data and work in our study, which is to analyze the peligrosity of the earthquakes in an arbitrary area of interest. \n",
    "\n",
    "For our repository we have decided so use the next convention: \"the first letter shows the level of the folders being \"A\" for the first level and \"B\" for the second level. Then, the numbers that follows the letter are used to organize the folders as we pleased. \n",
    "\n",
    "The scripts and folders regarded to the seismic events will have an \"eq_\" prefix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f72a5a8",
   "metadata": {},
   "source": [
    "The scripts are organized in the following way:\n",
    "\n",
    "-A00_data: This folder will be the one in charge to the download and storage of the data. For now, it only serves as a temporary storage, the database will eventually be stored in a more permanent way.\n",
    "\n",
    "-A01_scripts: This folder will contain the scripts that will be used to download and process the data. Here we can find the folder \"B01_eq_download\", where the script \"eq_download.py\" is located and the utils tha it uses. This script will download the data from the USGS Earthquake Catalog and store it in the folder \"A00_data\". There is also the directory \"B01_4_eq_processing\" where we have \"preprocess.py\" and \"process.py\". These scripts are the responsible to merge both data frames obtained previously and process the data.\n",
    "\n",
    "-A02_utils: here the general utils are stored, those that can be used for any script or that are going to be used to create the dashboard.\n",
    "\n",
    "-A03_performance:\n",
    "\n",
    "-A04_web: in this directory is the Quarto script that creates the dashboard.\n",
    "\n",
    "-A05_config:\n",
    "\n",
    "-A06_tests:\n",
    "\n",
    "-A07_docs: \n",
    "\n",
    "-A08_notebooks: where notebooks are stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c1cf5",
   "metadata": {},
   "source": [
    "# Functions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd83086f",
   "metadata": {},
   "source": [
    "## Functions from B01_2_eq_dowloand\n",
    "\n",
    "### eq_download.py\n",
    "\n",
    "    -search_by_min_magnitude(date_i, date_f, min_magn, center_coords, reg_rad)\n",
    "\n",
    "This function calls the libcomcat functions search(), get_summary_data_frame() and get_detail_data_frame(), to get the basic information of the events and their magnitudes information. \n",
    "\n",
    "Its parameters are: starting date (date_i), ending date (date_f), minimum magnitude of the events (min_mag), coordinates of the volcano we want to study (center_coords in form (latitude, longitude) list) and the radius of the area we want to study in km (reg_rad).\n",
    "\n",
    "The function returns a dataframe with the parameters that will be used to get the trigger index by merging the two dataframes obtained from the previous functions with the auxiliary function working_df().\n",
    "\n",
    "    -download_all_by_region(date_i, date_f, center_coords, reg_rad)\n",
    "\n",
    "This is the main function for the script since it requests to the USGS catalog the data for all earthquakes in the region with a minimum magnitude of 0.001, since we can be also interested in looking for the magnitude of completeness. \n",
    "\n",
    "It uses the same parameters as the previous function, but it does not require the minimum magnitude. The return is the same dataframe, but with an wider list of events.\n",
    "    \n",
    "    -working_df(df1, df2)\n",
    "\n",
    "As it is said before, this is an auxiliary function that is used to merge the two dataframes obtained from the libcomcat functions. It is necessary because the we can not know which type of magnitude is the function get_summary_data_frame() going to return. Thus, with this function we can merge, by the ID of the events, the two dataframes and get the informatio1n we need.\n",
    "\n",
    "### utils.py\n",
    "\n",
    "    -get_magnitude_type(mag_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f1e4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4bca7c2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
